{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment_utterance</th>\n",
       "      <th>sentiment_context_all</th>\n",
       "      <th>sentiment_context_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[I never would have identified the fingerprint...</td>\n",
       "      <td>[LEONARD, SHELDON]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.783, 'pos': 0.217, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>[This is one of my favorite places to kick bac...</td>\n",
       "      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.18, 'neu': 0.82, 'pos': 0.0, 'compou...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'comp...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n",
       "      <td>[LEONARD, HOWARD, LEONARD]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>{'neg': 0.143, 'neu': 0.857, 'pos': 0.0, 'comp...</td>\n",
       "      <td>[{'neg': 0.268, 'neu': 0.732, 'pos': 0.0, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[A marathon? How many Superman movies are ther...</td>\n",
       "      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>{'neg': 0.058, 'neu': 0.851, 'pos': 0.091, 'co...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1105</td>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Great Caesar's ghost, look at this place., So...</td>\n",
       "      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>{'neg': 0.097, 'neu': 0.815, 'pos': 0.088, 'co...</td>\n",
       "      <td>[{'neg': 0.202, 'neu': 0.439, 'pos': 0.36, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>2169</td>\n",
       "      <td>Hes not right for the part, and if I suggest h...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[What am I gonna do now?, Just pass the tape a...</td>\n",
       "      <td>[CHANDLER, RACHEL]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.102, 'neu': 0.898, 'pos': 0.0, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>2235</td>\n",
       "      <td>Oh yeah he has a caretaker his older brother, ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Helo! Anybody in there order a celebrity?, Wh...</td>\n",
       "      <td>[JOEY, PERSON, CHANDLER, PERSON]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>False</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.858, 'pos': 0.142, 'comp...</td>\n",
       "      <td>{'neg': 0.062, 'neu': 0.751, 'pos': 0.187, 'co...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>234</td>\n",
       "      <td>Is it me or the greetings gone downhill around...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Hey, You son of a bitch!]</td>\n",
       "      <td>[CHANDLER, JOEY]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'comp...</td>\n",
       "      <td>{'neg': 0.506, 'neu': 0.494, 'pos': 0.0, 'comp...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>2608</td>\n",
       "      <td>You are right, by saying nice, I am virtually ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Did I go to this school?, Hey, there's Missy ...</td>\n",
       "      <td>[CHANDLER, ROSS, CHANDLER, ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'comp...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>2524</td>\n",
       "      <td>Yes and we are \"very\" excited about it.</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Anyway, if you don't feel like being alone to...</td>\n",
       "      <td>[ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.527, 'pos': 0.473, 'comp...</td>\n",
       "      <td>{'neg': 0.159, 'neu': 0.736, 'pos': 0.105, 'co...</td>\n",
       "      <td>[{'neg': 0.159, 'neu': 0.736, 'pos': 0.105, 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                          utterance   speaker  \\\n",
       "0      160  It's just a privilege to watch your mind at work.   SHELDON   \n",
       "1      170  I don't think I'll be able to stop thinking ab...     PENNY   \n",
       "2      180  Since it's not bee season, you can have my epi...   SHELDON   \n",
       "3      190  Lois Lane is falling, accelerating at an initi...   SHELDON   \n",
       "4     1105  I'm just inferring this is a couch because the...   SHELDON   \n",
       "..     ...                                                ...       ...   \n",
       "685   2169  Hes not right for the part, and if I suggest h...  CHANDLER   \n",
       "686   2235  Oh yeah he has a caretaker his older brother, ...  CHANDLER   \n",
       "687    234  Is it me or the greetings gone downhill around...  CHANDLER   \n",
       "688   2608  You are right, by saying nice, I am virtually ...  CHANDLER   \n",
       "689   2524            Yes and we are \"very\" excited about it.  CHANDLER   \n",
       "\n",
       "                                               context  \\\n",
       "0    [I never would have identified the fingerprint...   \n",
       "1    [This is one of my favorite places to kick bac...   \n",
       "2    [Here we go. Pad thai, no peanuts., But does i...   \n",
       "3    [A marathon? How many Superman movies are ther...   \n",
       "4    [Great Caesar's ghost, look at this place., So...   \n",
       "..                                                 ...   \n",
       "685  [What am I gonna do now?, Just pass the tape a...   \n",
       "686  [Helo! Anybody in there order a celebrity?, Wh...   \n",
       "687                         [Hey, You son of a bitch!]   \n",
       "688  [Did I go to this school?, Hey, there's Missy ...   \n",
       "689  [Anyway, if you don't feel like being alone to...   \n",
       "\n",
       "                                      context_speakers     show sarcasm  \\\n",
       "0                                   [LEONARD, SHELDON]      BBT    True   \n",
       "1    [HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...      BBT    True   \n",
       "2                           [LEONARD, HOWARD, LEONARD]      BBT   False   \n",
       "3    [PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...      BBT   False   \n",
       "4    [SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...      BBT    True   \n",
       "..                                                 ...      ...     ...   \n",
       "685                                 [CHANDLER, RACHEL]  FRIENDS    True   \n",
       "686                   [JOEY, PERSON, CHANDLER, PERSON]  FRIENDS   False   \n",
       "687                                   [CHANDLER, JOEY]  FRIENDS    True   \n",
       "688                   [CHANDLER, ROSS, CHANDLER, ROSS]  FRIENDS    True   \n",
       "689                                             [ROSS]  FRIENDS    True   \n",
       "\n",
       "                                   sentiment_utterance  \\\n",
       "0    {'neg': 0.0, 'neu': 0.783, 'pos': 0.217, 'comp...   \n",
       "1    {'neg': 0.18, 'neu': 0.82, 'pos': 0.0, 'compou...   \n",
       "2    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3    {'neg': 0.058, 'neu': 0.851, 'pos': 0.091, 'co...   \n",
       "4    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "..                                                 ...   \n",
       "685  {'neg': 0.102, 'neu': 0.898, 'pos': 0.0, 'comp...   \n",
       "686  {'neg': 0.0, 'neu': 0.858, 'pos': 0.142, 'comp...   \n",
       "687  {'neg': 0.0, 'neu': 0.763, 'pos': 0.237, 'comp...   \n",
       "688  {'neg': 0.0, 'neu': 0.781, 'pos': 0.219, 'comp...   \n",
       "689  {'neg': 0.0, 'neu': 0.527, 'pos': 0.473, 'comp...   \n",
       "\n",
       "                                 sentiment_context_all  \\\n",
       "0    {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1    {'neg': 0.0, 'neu': 0.871, 'pos': 0.129, 'comp...   \n",
       "2    {'neg': 0.143, 'neu': 0.857, 'pos': 0.0, 'comp...   \n",
       "3    {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...   \n",
       "4    {'neg': 0.097, 'neu': 0.815, 'pos': 0.088, 'co...   \n",
       "..                                                 ...   \n",
       "685  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "686  {'neg': 0.062, 'neu': 0.751, 'pos': 0.187, 'co...   \n",
       "687  {'neg': 0.506, 'neu': 0.494, 'pos': 0.0, 'comp...   \n",
       "688  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "689  {'neg': 0.159, 'neu': 0.736, 'pos': 0.105, 'co...   \n",
       "\n",
       "                        sentiment_context_per_sentence  \n",
       "0    [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "1    [{'neg': 0.0, 'neu': 0.705, 'pos': 0.295, 'com...  \n",
       "2    [{'neg': 0.268, 'neu': 0.732, 'pos': 0.0, 'com...  \n",
       "3    [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "4    [{'neg': 0.202, 'neu': 0.439, 'pos': 0.36, 'co...  \n",
       "..                                                 ...  \n",
       "685  [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "686  [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "687  [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "688  [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compoun...  \n",
       "689  [{'neg': 0.159, 'neu': 0.736, 'pos': 0.105, 'c...  \n",
       "\n",
       "[690 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the JSON file into a DataFrame\n",
    "file_path = 'data/sarcasm_data.json'\n",
    "df = pd.read_json(file_path).transpose()\n",
    "\n",
    "# Reset the index to turn the first element into a new column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Define a function to apply sentiment analysis to a text\n",
    "def get_sentiment(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Apply sentiment analysis to the 'utterance' column\n",
    "df['sentiment_utterance'] = df['utterance'].apply(get_sentiment)\n",
    "\n",
    "# Apply sentiment analysis to the 'context' column\n",
    "df['sentiment_context_all'] = df['context'].apply(get_sentiment)\n",
    "\n",
    "# Apply sentiment analysis to each sentence in the 'context' column\n",
    "df['sentiment_context_per_sentence'] = df['context'].apply(lambda context: [get_sentiment(sentence) for sentence in context])\n",
    "\n",
    "# Visualize the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation format SA context and utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context all sequences: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "context per sequence: [{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}]\n",
      "utterance: {'neg': 0.0, 'neu': 0.783, 'pos': 0.217, 'compound': 0.3612}\n"
     ]
    }
   ],
   "source": [
    "df_context = df['sentiment_context_all']\n",
    "\n",
    "print(\"context all sequences:\", df['sentiment_context_all'][0])\n",
    "df['sentiment_context_all'][0]\n",
    "print(\"context per sequence:\", df['sentiment_context_per_sentence'][0])\n",
    "print(\"utterance:\", df['sentiment_utterance'][0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data for Training in format 1 sentiment for all context sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_neg</th>\n",
       "      <th>context_neu</th>\n",
       "      <th>context_pos</th>\n",
       "      <th>utterance_neg</th>\n",
       "      <th>utterance_neu</th>\n",
       "      <th>utterance_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.871</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.143</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.062</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.506</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0.159</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     context_neg  context_neu  context_pos  utterance_neg  utterance_neu  \\\n",
       "0          0.000        1.000        0.000          0.000          0.783   \n",
       "1          0.000        0.871        0.129          0.180          0.820   \n",
       "2          0.143        0.857        0.000          0.000          1.000   \n",
       "3          0.000        0.906        0.094          0.058          0.851   \n",
       "4          0.097        0.815        0.088          0.000          1.000   \n",
       "..           ...          ...          ...            ...            ...   \n",
       "685        0.000        1.000        0.000          0.102          0.898   \n",
       "686        0.062        0.751        0.187          0.000          0.858   \n",
       "687        0.506        0.494        0.000          0.000          0.763   \n",
       "688        0.000        1.000        0.000          0.000          0.781   \n",
       "689        0.159        0.736        0.105          0.000          0.527   \n",
       "\n",
       "     utterance_pos  \n",
       "0            0.217  \n",
       "1            0.000  \n",
       "2            0.000  \n",
       "3            0.091  \n",
       "4            0.000  \n",
       "..             ...  \n",
       "685          0.000  \n",
       "686          0.142  \n",
       "687          0.237  \n",
       "688          0.219  \n",
       "689          0.473  \n",
       "\n",
       "[690 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts sarcasm True to 1 and False to 0\n",
    "y = df['sarcasm'].astype(int).tolist()\n",
    "\n",
    "context_all = df[\"sentiment_context_all\"]\n",
    "context_sentences = df[\"sentiment_context_per_sentence\"]\n",
    "utterance = df[\"sentiment_utterance\"]\n",
    "\n",
    "# Extracting 'neg', 'neu', and 'pos' per sentence and add index to keep context \n",
    "X = np.array([ [context_all[i]['neg'], context_all[i]['neu'], context_all[i]['pos'], utterance[i]['neg'], utterance[i]['neu'], utterance[i]['pos']] for i in range(context_all.shape[0]) ])\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pd.DataFrame(X, columns=['context_neg', 'context_neu', 'context_pos', 'utterance_neg', 'utterance_neu', 'utterance_pos'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format DataFrame: 1 emotion per sentence + Utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ctx_3_neg</th>\n",
       "      <th>Ctx_3_neu</th>\n",
       "      <th>Ctx_3_pos</th>\n",
       "      <th>Ctx_2_neg</th>\n",
       "      <th>Ctx_2_neu</th>\n",
       "      <th>Ctx_2_pos</th>\n",
       "      <th>Ctx_1_neg</th>\n",
       "      <th>Ctx_1_neu</th>\n",
       "      <th>Ctx_1_pos</th>\n",
       "      <th>Utr_neg</th>\n",
       "      <th>Utr_neu</th>\n",
       "      <th>Utr_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.349</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>0.159</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ctx_3_neg  Ctx_3_neu  Ctx_3_pos  Ctx_2_neg  Ctx_2_neu  Ctx_2_pos  \\\n",
       "0        0.000      1.000      0.000      0.000      1.000      0.000   \n",
       "1        0.000      1.000      0.000      0.000      0.426      0.574   \n",
       "2        0.268      0.732      0.000      0.000      1.000      0.000   \n",
       "3        0.000      1.000      0.000      0.000      0.649      0.351   \n",
       "4        0.349      0.651      0.000      0.000      0.435      0.565   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "685      0.000      1.000      0.000      0.000      1.000      0.000   \n",
       "686      0.000      1.000      0.000      0.105      0.702      0.193   \n",
       "687      0.000      1.000      0.000      0.506      0.494      0.000   \n",
       "688      0.000      1.000      0.000      0.000      0.000      1.000   \n",
       "689      0.159      0.736      0.105      0.000      0.000      0.000   \n",
       "\n",
       "     Ctx_1_neg  Ctx_1_neu  Ctx_1_pos  Utr_neg  Utr_neu  Utr_pos  \n",
       "0        0.000      0.000       0.00    0.000    0.783    0.217  \n",
       "1        0.000      1.000       0.00    0.180    0.820    0.000  \n",
       "2        0.116      0.884       0.00    0.000    1.000    0.000  \n",
       "3        0.000      1.000       0.00    0.058    0.851    0.091  \n",
       "4        0.000      1.000       0.00    0.000    1.000    0.000  \n",
       "..         ...        ...        ...      ...      ...      ...  \n",
       "685      0.000      0.000       0.00    0.102    0.898    0.000  \n",
       "686      0.000      0.750       0.25    0.000    0.858    0.142  \n",
       "687      0.000      0.000       0.00    0.000    0.763    0.237  \n",
       "688      0.000      1.000       0.00    0.000    0.781    0.219  \n",
       "689      0.000      0.000       0.00    0.000    0.527    0.473  \n",
       "\n",
       "[690 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the keys for the floats\n",
    "keys = ['neg', 'neu', 'pos']\n",
    "\n",
    "# Initialize a list to hold the resulting arrays\n",
    "X_sentences = []\n",
    "\n",
    "# Process each array (list of dictionaries)\n",
    "for idx, sentences in enumerate(context_sentences):\n",
    "    # Ensure each list has at least 3 dictionaries, padding if necessary\n",
    "    sentences.extend([{'neg': 0.0, 'neu': 0.0, 'pos': 0.0}] * (3 - len(sentences)))\n",
    "\n",
    "    # Extract the relevant values from the last 3 context sentences and the corresponding utterance\n",
    "    context_values = [value for sentence in sentences[-3:] for value in (sentence['neg'], sentence['neu'], sentence['pos'])]\n",
    "    utterance_values = [utterance[idx][key] for key in keys]\n",
    "\n",
    "    # Combine the context and utterance values\n",
    "    X_sentences.append(context_values + utterance_values)\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_sentences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame with appropriate column names\n",
    "columns = [f\"Ctx_{i}_{key}\" for i in range(3, 0, -1) for key in keys] + [f\"Utr_{key}\" for key in keys]\n",
    "pd.DataFrame(X_sentences, columns=columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overal context score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "Confusion Matrix:\n",
      "[[42 11]\n",
      " [60 25]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.79      0.54        53\n",
      "           1       0.69      0.29      0.41        85\n",
      "\n",
      "    accuracy                           0.49       138\n",
      "   macro avg       0.55      0.54      0.48       138\n",
      "weighted avg       0.59      0.49      0.46       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context score per sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "Confusion Matrix:\n",
      "[[31 22]\n",
      " [49 36]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.58      0.47        53\n",
      "           1       0.62      0.42      0.50        85\n",
      "\n",
      "    accuracy                           0.49       138\n",
      "   macro avg       0.50      0.50      0.48       138\n",
      "weighted avg       0.53      0.49      0.49       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_all, y_train_all)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_all = model.predict(X_test_all)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_all, y_pred_all)\n",
    "conf_matrix = confusion_matrix(y_test_all, y_pred_all)\n",
    "report = classification_report(y_test_all, y_pred_all)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "def hartmann_sentiment(text):\n",
    "    text = classifier(text)\n",
    "    # Assuming 'text' is already defined and contains the necessary data\n",
    "    sentiments = [text[0][i]['score'] for i in range(6)]\n",
    "    sentiment = np.argmax(sentiments)\n",
    "    return sentiment\n",
    "text = \"I love this!\"\n",
    "hartmann_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentiment to each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment_utterance</th>\n",
       "      <th>sentiment_context_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[I never would have identified the fingerprint...</td>\n",
       "      <td>[LEONARD, SHELDON]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>[This is one of my favorite places to kick bac...</td>\n",
       "      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n",
       "      <td>[LEONARD, HOWARD, LEONARD]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[A marathon? How many Superman movies are ther...</td>\n",
       "      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Great Caesar's ghost, look at this place., So...</td>\n",
       "      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>Hes not right for the part, and if I suggest h...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[What am I gonna do now?, Just pass the tape a...</td>\n",
       "      <td>[CHANDLER, RACHEL]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>Oh yeah he has a caretaker his older brother, ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Helo! Anybody in there order a celebrity?, Wh...</td>\n",
       "      <td>[JOEY, PERSON, CHANDLER, PERSON]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Is it me or the greetings gone downhill around...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Hey, You son of a bitch!]</td>\n",
       "      <td>[CHANDLER, JOEY]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>You are right, by saying nice, I am virtually ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Did I go to this school?, Hey, there's Missy ...</td>\n",
       "      <td>[CHANDLER, ROSS, CHANDLER, ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>Yes and we are \"very\" excited about it.</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Anyway, if you don't feel like being alone to...</td>\n",
       "      <td>[ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              utterance   speaker  \\\n",
       "160   It's just a privilege to watch your mind at work.   SHELDON   \n",
       "170   I don't think I'll be able to stop thinking ab...     PENNY   \n",
       "180   Since it's not bee season, you can have my epi...   SHELDON   \n",
       "190   Lois Lane is falling, accelerating at an initi...   SHELDON   \n",
       "1105  I'm just inferring this is a couch because the...   SHELDON   \n",
       "...                                                 ...       ...   \n",
       "2169  Hes not right for the part, and if I suggest h...  CHANDLER   \n",
       "2235  Oh yeah he has a caretaker his older brother, ...  CHANDLER   \n",
       "234   Is it me or the greetings gone downhill around...  CHANDLER   \n",
       "2608  You are right, by saying nice, I am virtually ...  CHANDLER   \n",
       "2524            Yes and we are \"very\" excited about it.  CHANDLER   \n",
       "\n",
       "                                                context  \\\n",
       "160   [I never would have identified the fingerprint...   \n",
       "170   [This is one of my favorite places to kick bac...   \n",
       "180   [Here we go. Pad thai, no peanuts., But does i...   \n",
       "190   [A marathon? How many Superman movies are ther...   \n",
       "1105  [Great Caesar's ghost, look at this place., So...   \n",
       "...                                                 ...   \n",
       "2169  [What am I gonna do now?, Just pass the tape a...   \n",
       "2235  [Helo! Anybody in there order a celebrity?, Wh...   \n",
       "234                          [Hey, You son of a bitch!]   \n",
       "2608  [Did I go to this school?, Hey, there's Missy ...   \n",
       "2524  [Anyway, if you don't feel like being alone to...   \n",
       "\n",
       "                                       context_speakers     show sarcasm  \\\n",
       "160                                  [LEONARD, SHELDON]      BBT    True   \n",
       "170   [HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...      BBT    True   \n",
       "180                          [LEONARD, HOWARD, LEONARD]      BBT   False   \n",
       "190   [PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...      BBT   False   \n",
       "1105  [SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...      BBT    True   \n",
       "...                                                 ...      ...     ...   \n",
       "2169                                 [CHANDLER, RACHEL]  FRIENDS    True   \n",
       "2235                   [JOEY, PERSON, CHANDLER, PERSON]  FRIENDS   False   \n",
       "234                                    [CHANDLER, JOEY]  FRIENDS    True   \n",
       "2608                   [CHANDLER, ROSS, CHANDLER, ROSS]  FRIENDS    True   \n",
       "2524                                             [ROSS]  FRIENDS    True   \n",
       "\n",
       "      sentiment_utterance  sentiment_context_all  \n",
       "160                     4                      4  \n",
       "170                     4                      4  \n",
       "180                     4                      4  \n",
       "190                     4                      4  \n",
       "1105                    4                      2  \n",
       "...                   ...                    ...  \n",
       "2169                    0                      0  \n",
       "2235                    1                      0  \n",
       "234                     4                      4  \n",
       "2608                    1                      4  \n",
       "2524                    3                      5  \n",
       "\n",
       "[690 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "file_path = 'data/sarcasm_data.json'\n",
    "df_h = pd.read_json(file_path).transpose()\n",
    "\n",
    "\n",
    "# Apply sentiment analysis to the 'utterance' column\n",
    "df_h['sentiment_utterance'] = df_h['utterance'].apply(hartmann_sentiment)\n",
    "\n",
    "# Apply sentiment analysis to the 'context' column\n",
    "df_h['sentiment_context_all'] = df_h['context'].apply(hartmann_sentiment)\n",
    "\n",
    "\n",
    "# Visualize the DataFrame\n",
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[I never would have identified the fingerprint...</td>\n",
       "      <td>[LEONARD, SHELDON]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>[This is one of my favorite places to kick bac...</td>\n",
       "      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n",
       "      <td>[LEONARD, HOWARD, LEONARD]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[A marathon? How many Superman movies are ther...</td>\n",
       "      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Great Caesar's ghost, look at this place., So...</td>\n",
       "      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>Hes not right for the part, and if I suggest h...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[What am I gonna do now?, Just pass the tape a...</td>\n",
       "      <td>[CHANDLER, RACHEL]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>Oh yeah he has a caretaker his older brother, ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Helo! Anybody in there order a celebrity?, Wh...</td>\n",
       "      <td>[JOEY, PERSON, CHANDLER, PERSON]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Is it me or the greetings gone downhill around...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Hey, You son of a bitch!]</td>\n",
       "      <td>[CHANDLER, JOEY]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>You are right, by saying nice, I am virtually ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Did I go to this school?, Hey, there's Missy ...</td>\n",
       "      <td>[CHANDLER, ROSS, CHANDLER, ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>Yes and we are \"very\" excited about it.</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Anyway, if you don't feel like being alone to...</td>\n",
       "      <td>[ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              utterance   speaker  \\\n",
       "160   It's just a privilege to watch your mind at work.   SHELDON   \n",
       "170   I don't think I'll be able to stop thinking ab...     PENNY   \n",
       "180   Since it's not bee season, you can have my epi...   SHELDON   \n",
       "190   Lois Lane is falling, accelerating at an initi...   SHELDON   \n",
       "1105  I'm just inferring this is a couch because the...   SHELDON   \n",
       "...                                                 ...       ...   \n",
       "2169  Hes not right for the part, and if I suggest h...  CHANDLER   \n",
       "2235  Oh yeah he has a caretaker his older brother, ...  CHANDLER   \n",
       "234   Is it me or the greetings gone downhill around...  CHANDLER   \n",
       "2608  You are right, by saying nice, I am virtually ...  CHANDLER   \n",
       "2524            Yes and we are \"very\" excited about it.  CHANDLER   \n",
       "\n",
       "                                                context  \\\n",
       "160   [I never would have identified the fingerprint...   \n",
       "170   [This is one of my favorite places to kick bac...   \n",
       "180   [Here we go. Pad thai, no peanuts., But does i...   \n",
       "190   [A marathon? How many Superman movies are ther...   \n",
       "1105  [Great Caesar's ghost, look at this place., So...   \n",
       "...                                                 ...   \n",
       "2169  [What am I gonna do now?, Just pass the tape a...   \n",
       "2235  [Helo! Anybody in there order a celebrity?, Wh...   \n",
       "234                          [Hey, You son of a bitch!]   \n",
       "2608  [Did I go to this school?, Hey, there's Missy ...   \n",
       "2524  [Anyway, if you don't feel like being alone to...   \n",
       "\n",
       "                                       context_speakers     show sarcasm  \n",
       "160                                  [LEONARD, SHELDON]      BBT    True  \n",
       "170   [HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...      BBT    True  \n",
       "180                          [LEONARD, HOWARD, LEONARD]      BBT   False  \n",
       "190   [PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...      BBT   False  \n",
       "1105  [SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...      BBT    True  \n",
       "...                                                 ...      ...     ...  \n",
       "2169                                 [CHANDLER, RACHEL]  FRIENDS    True  \n",
       "2235                   [JOEY, PERSON, CHANDLER, PERSON]  FRIENDS   False  \n",
       "234                                    [CHANDLER, JOEY]  FRIENDS    True  \n",
       "2608                   [CHANDLER, ROSS, CHANDLER, ROSS]  FRIENDS    True  \n",
       "2524                                             [ROSS]  FRIENDS    True  \n",
       "\n",
       "[690 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "file_path = 'data/sarcasm_data.json'\n",
    "df_h = pd.read_json(file_path).transpose()\n",
    "df_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sentiment for 3 context sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to each sentence in the 'context' column\n",
    "df_h['sentiment_context_per_sentence'] = df_h['context'].apply(lambda context: [hartmann_sentiment(sentence) for sentence in context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment_utterance</th>\n",
       "      <th>sentiment_context_all</th>\n",
       "      <th>sentiment_context_per_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[I never would have identified the fingerprint...</td>\n",
       "      <td>[LEONARD, SHELDON]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>[This is one of my favorite places to kick bac...</td>\n",
       "      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n",
       "      <td>[LEONARD, HOWARD, LEONARD]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[A marathon? How many Superman movies are ther...</td>\n",
       "      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 0, 4, 4, 4, 4, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Great Caesar's ghost, look at this place., So...</td>\n",
       "      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[2, 1, 1, 1, 0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>Hes not right for the part, and if I suggest h...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[What am I gonna do now?, Just pass the tape a...</td>\n",
       "      <td>[CHANDLER, RACHEL]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>Oh yeah he has a caretaker his older brother, ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Helo! Anybody in there order a celebrity?, Wh...</td>\n",
       "      <td>[JOEY, PERSON, CHANDLER, PERSON]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 4, 1, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Is it me or the greetings gone downhill around...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Hey, You son of a bitch!]</td>\n",
       "      <td>[CHANDLER, JOEY]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>You are right, by saying nice, I am virtually ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Did I go to this school?, Hey, there's Missy ...</td>\n",
       "      <td>[CHANDLER, ROSS, CHANDLER, ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 4, 3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>Yes and we are \"very\" excited about it.</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Anyway, if you don't feel like being alone to...</td>\n",
       "      <td>[ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              utterance   speaker  \\\n",
       "160   It's just a privilege to watch your mind at work.   SHELDON   \n",
       "170   I don't think I'll be able to stop thinking ab...     PENNY   \n",
       "180   Since it's not bee season, you can have my epi...   SHELDON   \n",
       "190   Lois Lane is falling, accelerating at an initi...   SHELDON   \n",
       "1105  I'm just inferring this is a couch because the...   SHELDON   \n",
       "...                                                 ...       ...   \n",
       "2169  Hes not right for the part, and if I suggest h...  CHANDLER   \n",
       "2235  Oh yeah he has a caretaker his older brother, ...  CHANDLER   \n",
       "234   Is it me or the greetings gone downhill around...  CHANDLER   \n",
       "2608  You are right, by saying nice, I am virtually ...  CHANDLER   \n",
       "2524            Yes and we are \"very\" excited about it.  CHANDLER   \n",
       "\n",
       "                                                context  \\\n",
       "160   [I never would have identified the fingerprint...   \n",
       "170   [This is one of my favorite places to kick bac...   \n",
       "180   [Here we go. Pad thai, no peanuts., But does i...   \n",
       "190   [A marathon? How many Superman movies are ther...   \n",
       "1105  [Great Caesar's ghost, look at this place., So...   \n",
       "...                                                 ...   \n",
       "2169  [What am I gonna do now?, Just pass the tape a...   \n",
       "2235  [Helo! Anybody in there order a celebrity?, Wh...   \n",
       "234                          [Hey, You son of a bitch!]   \n",
       "2608  [Did I go to this school?, Hey, there's Missy ...   \n",
       "2524  [Anyway, if you don't feel like being alone to...   \n",
       "\n",
       "                                       context_speakers     show sarcasm  \\\n",
       "160                                  [LEONARD, SHELDON]      BBT    True   \n",
       "170   [HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...      BBT    True   \n",
       "180                          [LEONARD, HOWARD, LEONARD]      BBT   False   \n",
       "190   [PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...      BBT   False   \n",
       "1105  [SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...      BBT    True   \n",
       "...                                                 ...      ...     ...   \n",
       "2169                                 [CHANDLER, RACHEL]  FRIENDS    True   \n",
       "2235                   [JOEY, PERSON, CHANDLER, PERSON]  FRIENDS   False   \n",
       "234                                    [CHANDLER, JOEY]  FRIENDS    True   \n",
       "2608                   [CHANDLER, ROSS, CHANDLER, ROSS]  FRIENDS    True   \n",
       "2524                                             [ROSS]  FRIENDS    True   \n",
       "\n",
       "      sentiment_utterance  sentiment_context_all  \\\n",
       "160                     4                      4   \n",
       "170                     4                      4   \n",
       "180                     4                      4   \n",
       "190                     4                      4   \n",
       "1105                    4                      2   \n",
       "...                   ...                    ...   \n",
       "2169                    0                      0   \n",
       "2235                    1                      0   \n",
       "234                     4                      4   \n",
       "2608                    1                      4   \n",
       "2524                    3                      5   \n",
       "\n",
       "     sentiment_context_per_sentence  \n",
       "160                          [4, 5]  \n",
       "170           [4, 4, 4, 4, 4, 4, 4]  \n",
       "180                       [4, 4, 4]  \n",
       "190           [4, 0, 4, 4, 4, 4, 4]  \n",
       "1105             [2, 1, 1, 1, 0, 4]  \n",
       "...                             ...  \n",
       "2169                         [0, 4]  \n",
       "2235                   [0, 4, 1, 4]  \n",
       "234                          [4, 0]  \n",
       "2608                   [4, 4, 3, 1]  \n",
       "2524                            [5]  \n",
       "\n",
       "[690 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5, -1], [4, 4, 4], [4, 4, 4], [4, 0, 4], [2, 1, 1], [4, 4, 4], [3, 3, -1], [4, 4, 4], [5, 4, 1], [1, 0, 0], [5, 4, 4], [4, 4, 0], [4, 5, -1], [4, 5, 5], [1, 1, 4], [4, 0, 4], [0, 4, 4], [4, 4, 3], [3, 4, 4], [3, 4, 4], [4, -1, -1], [0, 0, 0], [4, 0, 0], [0, 4, 5], [0, 0, 4], [1, 4, 1], [5, 4, -1], [4, 4, 4], [4, 4, 4], [4, 0, 1], [4, 4, -1], [1, 0, -1], [4, 5, -1], [4, 0, 0], [4, 4, 3], [3, 4, 4], [4, 0, -1], [0, 4, -1], [4, 1, 4], [4, 4, 4], [0, 4, 4], [4, 0, -1], [4, 4, 4], [4, 4, 4], [4, 3, 3], [4, 0, 3], [4, 5, 4], [4, 4, 4], [0, 4, 4], [5, 1, 4], [4, 4, 4], [4, 4, 4], [4, 4, 0], [3, -1, -1], [4, 4, 4], [1, 2, 4], [3, 4, 4], [4, 5, 4], [4, 4, 0], [2, 4, 4], [4, 1, 4], [2, 4, 3], [4, 5, -1], [4, 0, 1], [4, 4, 4], [4, 4, 4], [0, 4, 4], [3, 4, 4], [4, 0, 0], [4, -1, -1], [0, 4, -1], [0, -1, -1], [4, 4, 4], [4, 4, 1], [4, 4, 1], [4, 4, 0], [3, 3, 3], [4, 4, 4], [3, 4, 4], [4, 3, 4], [4, 4, 3], [0, 0, 4], [3, 4, -1], [4, 4, -1], [3, 4, 1], [4, 4, 1], [4, 4, 4], [3, 4, 1], [4, 4, 5], [4, 3, 3], [4, 0, 4], [0, 4, 0], [1, 4, 4], [3, 4, 4], [5, 4, 4], [2, 1, 4], [3, 4, 3], [0, 2, -1], [4, 0, 4], [4, 0, 0], [4, 4, -1], [4, 4, 4], [4, 4, 4], [3, 4, 4], [4, 3, 5], [1, 0, -1], [4, 1, -1], [4, 3, 4], [4, 4, 4], [3, 4, -1], [0, 4, 0], [5, 4, 5], [4, 5, 4], [4, 4, 4], [0, 4, 4], [4, 4, 4], [2, 3, 1], [3, 4, 4], [4, 0, 4], [0, 4, 4], [2, 1, 4], [0, 0, 4], [4, 4, 0], [4, 5, 4], [1, 4, 1], [1, 4, 3], [5, 3, 1], [4, 4, 4], [4, 4, 4], [4, 4, 4], [1, 4, 1], [4, 4, 4], [4, 4, 4], [0, 3, 3], [4, 4, 1], [0, 3, 4], [4, 4, 1], [4, 4, 1], [4, 4, 4], [4, 4, 4], [4, 1, 1], [0, 1, 4], [4, 4, 4], [4, 2, 4], [5, 4, 4], [4, 0, 4], [1, 4, 4], [4, 4, 1], [1, 1, 4], [4, 4, 4], [0, 2, 4], [4, -1, -1], [3, -1, -1], [4, 4, 4], [4, 4, 1], [1, 4, 1], [4, 3, -1], [0, 4, 4], [1, 0, 4], [4, 1, 4], [3, 3, 5], [4, 4, 1], [4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 4], [0, 1, 1], [1, 3, 3], [3, 3, 3], [4, 4, 4], [4, 3, 3], [4, 2, 4], [4, 4, 1], [1, 5, 1], [4, 1, 1], [5, 4, 4], [3, 0, 4], [2, 4, 1], [4, 1, 4], [4, 4, 4], [1, 4, 2], [4, 4, 4], [4, 4, 4], [3, 3, 4], [4, 0, 4], [4, 4, 4], [1, 4, 2], [1, 5, 0], [1, 4, 4], [4, -1, -1], [0, 4, 4], [0, 1, 4], [4, 4, 4], [4, 3, 0], [4, 4, 3], [0, 1, -1], [0, 4, 0], [3, -1, -1], [4, 0, 4], [4, 0, 0], [5, 4, 5], [2, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 1], [0, -1, -1], [4, 3, 0], [3, 5, 3], [1, 4, 4], [0, 4, 4], [4, 4, 4], [4, 4, 4], [3, 3, 4], [4, 4, 4], [4, 3, 0], [4, 4, 1], [4, 5, 2], [0, 4, 5], [4, 4, -1], [4, 3, 4], [4, 4, 3], [4, 4, 4], [4, 3, 1], [4, 0, 1], [1, 1, 0], [1, 1, 0], [4, 4, 4], [3, 4, 3], [4, 4, 4], [4, 0, 4], [4, 4, 5], [0, 5, 0], [4, 4, 4], [4, 4, 5], [4, 4, 1], [4, 4, 0], [4, 1, 4], [4, 1, 1], [5, 0, 0], [4, 4, 4], [3, 4, 1], [1, 4, 4], [4, 4, 4], [1, 0, 5], [0, -1, -1], [3, 4, 4], [1, 0, 4], [3, 3, 3], [1, 4, 4], [2, 3, 4], [4, 1, 3], [4, 4, 5], [1, -1, -1], [4, 5, 4], [4, 1, 4], [4, 4, 4], [1, 4, 4], [4, 4, 4], [4, 0, 4], [4, 1, 4], [0, 1, 4], [0, 4, 4], [4, 4, 0], [1, 5, 4], [4, 4, 4], [4, 4, 0], [3, 4, 3], [4, 3, 4], [0, 4, 4], [4, -1, -1], [4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 4], [4, 4, 0], [5, 4, -1], [4, 3, 4], [3, 0, 0], [4, 4, 0], [4, -1, -1], [4, -1, -1], [1, 4, 4], [4, 4, 4], [4, -1, -1], [3, 4, -1], [4, -1, -1], [4, 3, -1], [5, 4, -1], [4, 4, 5], [3, -1, -1], [4, 4, 4], [4, 4, 0], [4, 4, 3], [4, -1, -1], [1, -1, -1], [4, -1, -1], [4, -1, -1], [4, 4, 0], [4, -1, -1], [4, -1, -1], [3, 4, 2], [5, -1, -1], [4, -1, -1], [4, -1, -1], [4, 0, 1], [5, 1, 4], [1, 4, -1], [0, -1, -1], [4, 4, 4], [5, 4, 0], [3, 4, -1], [4, 4, 4], [3, 3, -1], [4, -1, -1], [4, 4, 0], [4, -1, -1], [4, 4, 4], [0, -1, -1], [4, -1, -1], [0, 5, -1], [4, 4, 0], [4, -1, -1], [4, -1, -1], [2, -1, -1], [4, -1, -1], [5, -1, -1], [5, -1, -1], [4, 4, 0], [0, 4, 4], [3, 4, -1], [5, 4, 5], [4, -1, -1], [4, 4, 5], [0, -1, -1], [4, -1, -1], [4, -1, -1], [0, 4, -1], [4, 4, -1], [4, -1, -1], [3, 0, 2], [0, 4, -1], [5, 2, -1], [2, 0, 0], [2, 0, -1], [0, -1, -1], [4, 4, 3], [3, -1, -1], [3, -1, -1], [0, -1, -1], [4, 4, -1], [1, 0, -1], [4, -1, -1], [4, 0, -1], [0, 4, 0], [4, -1, -1], [4, 4, -1], [4, 4, 4], [4, -1, -1], [5, -1, -1], [3, 1, 1], [4, -1, -1], [1, -1, -1], [4, 0, 1], [1, -1, -1], [4, -1, -1], [1, 0, -1], [4, -1, -1], [4, -1, -1], [4, -1, -1], [1, -1, -1], [4, -1, -1], [3, -1, -1], [5, 4, -1], [4, -1, -1], [3, -1, -1], [4, -1, -1], [4, -1, -1], [4, 4, -1], [4, -1, -1], [0, 4, -1], [4, 0, 4], [0, 4, -1], [4, -1, -1], [4, 0, 4], [1, 4, -1], [3, -1, -1], [4, 4, -1], [3, 4, 0], [4, 4, 4], [0, 2, 2], [1, 2, 5], [3, 4, -1], [3, 3, 4], [4, 0, 4], [0, -1, -1], [4, 2, 5], [4, 4, 0], [4, 4, 0], [3, 3, 0], [4, 3, -1], [3, 3, 3], [0, 3, 4], [3, 4, 4], [4, 4, 4], [0, -1, -1], [1, 4, 4], [4, 4, 4], [1, 1, 4], [0, 1, 3], [3, -1, -1], [1, 4, -1], [4, 0, 5], [0, 4, 1], [3, 5, 1], [2, 3, 3], [4, 0, 0], [4, 1, -1], [3, 3, -1], [1, 5, -1], [1, 4, 4], [4, -1, -1], [1, 0, -1], [1, 3, 0], [4, 4, 3], [4, 0, 1], [0, 0, 0], [3, 1, 4], [4, 4, 3], [4, 4, 3], [0, 4, 4], [0, -1, -1], [0, -1, -1], [4, 0, 4], [0, 3, 1], [3, 3, -1], [4, 0, 4], [4, 0, 0], [4, 5, -1], [4, 0, 4], [4, 4, 3], [0, 4, 0], [1, -1, -1], [0, 0, 3], [1, -1, -1], [2, 4, 4], [1, -1, -1], [3, -1, -1], [3, 3, -1], [4, 4, 4], [4, 4, 4], [4, 5, -1], [3, -1, -1], [4, 3, 4], [1, 0, 4], [4, 0, 4], [3, -1, -1], [4, 0, -1], [1, 0, 4], [0, -1, -1], [3, 2, 0], [3, 3, 3], [1, -1, -1], [4, 0, 0], [4, 4, 4], [4, 3, 4], [2, 0, -1], [1, 5, 0], [4, 2, 4], [4, 3, 4], [0, 4, -1], [4, 4, 4], [4, 3, 4], [0, 0, 0], [3, 3, 3], [5, 4, 4], [1, 4, 4], [4, 4, 1], [4, 4, 0], [4, 0, 1], [3, 4, -1], [0, 4, 5], [3, -1, -1], [4, 5, 1], [0, -1, -1], [4, 4, 3], [4, 4, 3], [4, 4, 4], [5, -1, -1], [3, 3, 4], [4, 0, -1], [4, 4, 0], [0, 4, -1], [4, -1, -1], [4, 3, 4], [4, 4, 4], [0, -1, -1], [1, 0, 0], [4, 4, -1], [4, 1, 4], [0, 0, 1], [4, 4, -1], [3, 1, 3], [4, 0, 4], [5, -1, -1], [0, 0, 1], [0, 4, 0], [3, -1, -1], [0, 1, 4], [0, 1, 4], [4, 0, -1], [0, 1, -1], [0, -1, -1], [0, 3, -1], [3, 2, 4], [0, 1, -1], [1, 3, -1], [4, 2, 1], [4, 0, 0], [2, 2, 4], [3, 0, 4], [0, 4, 0], [0, 3, 4], [1, 4, 4], [5, 4, -1], [4, 4, 4], [3, 4, 4], [4, 4, 0], [3, 3, 4], [0, 5, 4], [0, 1, 0], [4, 5, -1], [4, 5, 4], [3, 4, 0], [3, -1, -1], [4, 0, 3], [4, 0, 0], [1, 3, 3], [4, 3, 4], [4, 5, 0], [4, 4, 4], [4, 4, -1], [2, 3, -1], [4, 4, 4], [4, 1, -1], [4, 4, 5], [4, 4, -1], [4, 4, 4], [1, 0, 3], [1, -1, -1], [4, 4, -1], [4, 0, -1], [2, 0, 0], [4, 0, -1], [3, 3, 4], [4, 0, 3], [1, 3, 4], [4, 2, -1], [3, -1, -1], [4, 4, -1], [4, 4, -1], [0, 5, 0], [0, 4, 3], [4, 4, -1], [3, 4, -1], [4, -1, -1], [3, 3, 4], [4, -1, -1], [4, -1, -1], [1, 1, -1], [5, -1, -1], [1, -1, -1], [3, 0, 3], [4, 4, 4], [4, 4, -1], [0, -1, -1], [4, 4, 1], [4, 3, 4], [4, 4, 5], [2, 4, 4], [4, -1, -1], [5, -1, -1], [1, 0, 0], [4, -1, -1], [4, 5, 0], [0, 0, 0], [4, -1, -1], [1, 3, 3], [0, 4, 4], [5, 0, 1], [4, 4, 2], [4, -1, -1], [0, 4, -1], [4, 4, -1], [4, -1, -1], [4, 3, -1], [3, 3, -1], [4, 0, 3], [3, 4, 4], [4, 0, -1], [5, -1, -1], [4, -1, -1], [3, 1, 4], [4, 3, 0], [0, 5, 3], [0, 1, 4], [3, 0, 1], [4, 1, -1], [0, 4, -1], [1, 4, -1], [3, 0, 0], [4, 4, 4], [5, 5, -1], [4, 4, 2], [4, -1, -1], [5, 3, 4], [3, 4, 5], [4, 0, -1], [1, 4, -1], [4, 0, -1], [5, 0, -1], [5, -1, -1], [4, 1, -1], [4, -1, -1], [4, 4, -1], [4, 2, -1], [0, 4, -1], [4, -1, -1], [4, 4, -1], [5, 0, -1], [4, 0, -1], [2, 1, -1], [4, 5, 0], [2, -1, -1], [4, 0, -1], [4, 0, -1], [4, -1, -1], [2, -1, -1], [4, 4, -1], [0, 0, -1], [5, 4, -1], [4, 4, -1], [4, 3, 4], [4, 4, 4], [5, 4, -1], [4, 0, 2], [4, -1, -1], [4, -1, -1], [0, 2, 4], [4, 0, -1], [4, 4, -1], [4, -1, -1], [4, 4, 4], [4, -1, -1], [5, -1, -1], [4, -1, -1], [1, -1, -1], [3, -1, -1], [0, 4, 0], [4, 1, -1], [1, -1, -1], [0, -1, -1], [5, -1, -1], [4, 4, 4], [1, -1, -1], [1, -1, -1], [0, -1, -1], [4, -1, -1], [5, 4, 4], [4, 4, -1], [4, -1, -1], [3, -1, -1], [4, -1, -1], [3, -1, -1], [4, -1, -1], [4, 4, -1], [4, -1, -1], [5, -1, -1], [0, -1, -1], [4, 3, 4], [5, -1, -1], [3, 4, 4], [4, 4, 4], [0, 4, -1], [0, 4, -1], [0, 4, 1], [4, 0, -1], [4, 4, 3], [5, -1, -1]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sentiment_utterance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment_utterance'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCtx_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCtx_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCtx_3\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     28\u001b[0m df_h_3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_sentences, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[1;32m---> 29\u001b[0m df_h_3 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_h_3, \u001b[43mdf_h\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentiment_utterance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m df_h_3\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentiment_utterance'"
     ]
    }
   ],
   "source": [
    "# Define the keys for the floats\n",
    "#keys = ['neg', 'neu', 'pos']\n",
    "\n",
    "# Initialize a list to hold the resulting arrays\n",
    "X_sentences = []\n",
    "sentiment_per_context = df_h['sentiment_context_per_sentence']\n",
    "# Process each array (list of dictionaries)\n",
    "for idx, sentence in enumerate(sentiment_per_context):\n",
    "    # Ensure each list has at least 3 dictionaries, padding if necessary\n",
    "    sentence.extend([-1] * (3 - len(sentence)))# Classify as -1 if no context sentence\n",
    "    sentence = sentence[:3]\n",
    "    X_sentences.append(sentence)\n",
    "\n",
    "print(X_sentences)\n",
    "\n",
    "    # Extract the relevant values from the last 3 context sentences and the corresponding utterance\n",
    "    #context_values = [value for sentence in sentences[-3:] for value in (sentence['neg'], sentence['neu'], sentence['pos'])]\n",
    "    #utterance_values = [utterance[idx][key] for key in keys]\n",
    "\n",
    "    # Combine the context and utterance values\n",
    "    #X_sentences.append(context_values + utterance_values)\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "#X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_sentences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DataFrame with appropriate column names\n",
    "columns = [\"Ctx_1\", \"Ctx_2\", \"Ctx_3\"]\n",
    "df_h_3 = pd.DataFrame(X_sentences, columns=columns)\n",
    "#df_h_3 = pd.concat([df_h_3, df_h['sentiment_utterance']], axis=1)\n",
    "df_h_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hartmann_sentiment(df_h['utterance'][160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file into a DataFrame\n",
    "file_path = 'data/sarcasm_data.json'\n",
    "df_h = pd.read_json(file_path).transpose()\n",
    "# Apply sentiment analysis to the 'utterance' column\n",
    "df_h['sentiment_utterance'] = df_h['utterance'].apply(hartmann_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "      <th>context</th>\n",
       "      <th>context_speakers</th>\n",
       "      <th>show</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>It's just a privilege to watch your mind at work.</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[I never would have identified the fingerprint...</td>\n",
       "      <td>[LEONARD, SHELDON]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>I don't think I'll be able to stop thinking ab...</td>\n",
       "      <td>PENNY</td>\n",
       "      <td>[This is one of my favorite places to kick bac...</td>\n",
       "      <td>[HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Since it's not bee season, you can have my epi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Here we go. Pad thai, no peanuts., But does i...</td>\n",
       "      <td>[LEONARD, HOWARD, LEONARD]</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Lois Lane is falling, accelerating at an initi...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[A marathon? How many Superman movies are ther...</td>\n",
       "      <td>[PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>I'm just inferring this is a couch because the...</td>\n",
       "      <td>SHELDON</td>\n",
       "      <td>[Great Caesar's ghost, look at this place., So...</td>\n",
       "      <td>[SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...</td>\n",
       "      <td>BBT</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>Hes not right for the part, and if I suggest h...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[What am I gonna do now?, Just pass the tape a...</td>\n",
       "      <td>[CHANDLER, RACHEL]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>Oh yeah he has a caretaker his older brother, ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Helo! Anybody in there order a celebrity?, Wh...</td>\n",
       "      <td>[JOEY, PERSON, CHANDLER, PERSON]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Is it me or the greetings gone downhill around...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Hey, You son of a bitch!]</td>\n",
       "      <td>[CHANDLER, JOEY]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>You are right, by saying nice, I am virtually ...</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Did I go to this school?, Hey, there's Missy ...</td>\n",
       "      <td>[CHANDLER, ROSS, CHANDLER, ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>Yes and we are \"very\" excited about it.</td>\n",
       "      <td>CHANDLER</td>\n",
       "      <td>[Anyway, if you don't feel like being alone to...</td>\n",
       "      <td>[ROSS]</td>\n",
       "      <td>FRIENDS</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              utterance   speaker  \\\n",
       "160   It's just a privilege to watch your mind at work.   SHELDON   \n",
       "170   I don't think I'll be able to stop thinking ab...     PENNY   \n",
       "180   Since it's not bee season, you can have my epi...   SHELDON   \n",
       "190   Lois Lane is falling, accelerating at an initi...   SHELDON   \n",
       "1105  I'm just inferring this is a couch because the...   SHELDON   \n",
       "...                                                 ...       ...   \n",
       "2169  Hes not right for the part, and if I suggest h...  CHANDLER   \n",
       "2235  Oh yeah he has a caretaker his older brother, ...  CHANDLER   \n",
       "234   Is it me or the greetings gone downhill around...  CHANDLER   \n",
       "2608  You are right, by saying nice, I am virtually ...  CHANDLER   \n",
       "2524            Yes and we are \"very\" excited about it.  CHANDLER   \n",
       "\n",
       "                                                context  \\\n",
       "160   [I never would have identified the fingerprint...   \n",
       "170   [This is one of my favorite places to kick bac...   \n",
       "180   [Here we go. Pad thai, no peanuts., But does i...   \n",
       "190   [A marathon? How many Superman movies are ther...   \n",
       "1105  [Great Caesar's ghost, look at this place., So...   \n",
       "...                                                 ...   \n",
       "2169  [What am I gonna do now?, Just pass the tape a...   \n",
       "2235  [Helo! Anybody in there order a celebrity?, Wh...   \n",
       "234                          [Hey, You son of a bitch!]   \n",
       "2608  [Did I go to this school?, Hey, there's Missy ...   \n",
       "2524  [Anyway, if you don't feel like being alone to...   \n",
       "\n",
       "                                       context_speakers     show sarcasm  \\\n",
       "160                                  [LEONARD, SHELDON]      BBT    True   \n",
       "170   [HOWARD, PENNY, HOWARD, HOWARD, HOWARD, PENNY,...      BBT    True   \n",
       "180                          [LEONARD, HOWARD, LEONARD]      BBT   False   \n",
       "190   [PENNY, SHELDON, PENNY, SHELDON, SHELDON, PENN...      BBT   False   \n",
       "1105  [SHELDON, LEONARD, SHELDON, SHELDON, SHELDON, ...      BBT    True   \n",
       "...                                                 ...      ...     ...   \n",
       "2169                                 [CHANDLER, RACHEL]  FRIENDS    True   \n",
       "2235                   [JOEY, PERSON, CHANDLER, PERSON]  FRIENDS   False   \n",
       "234                                    [CHANDLER, JOEY]  FRIENDS    True   \n",
       "2608                   [CHANDLER, ROSS, CHANDLER, ROSS]  FRIENDS    True   \n",
       "2524                                             [ROSS]  FRIENDS    True   \n",
       "\n",
       "      sentiment_utterance  \n",
       "160                     4  \n",
       "170                     4  \n",
       "180                     4  \n",
       "190                     4  \n",
       "1105                    4  \n",
       "...                   ...  \n",
       "2169                    0  \n",
       "2235                    1  \n",
       "234                     4  \n",
       "2608                    1  \n",
       "2524                    3  \n",
       "\n",
       "[690 rows x 7 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
